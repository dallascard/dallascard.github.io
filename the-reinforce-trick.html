<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>The REINFORCE trick &mdash; Reasonable Explanations</title>
  <meta name="author" content="Dallas Card">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="https://dallascard.github.io/favicon.png" rel="icon">

  <link href="https://dallascard.github.io/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="https://dallascard.github.io/">Reasonable Explanations</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>


<ul class="main-navigation">
      <li class="active">
        <a href="https://dallascard.github.io/category/archive.html">Archive</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">The REINFORCE trick</h1>
    <p class="meta">
<time datetime="2017-03-19T12:00:00-07:00" pubdate>Sun 19 March 2017</time>    </p>
</header>

  <div class="entry-content"><h2>Background</h2>
<p>There is a lot of excitement these days around reinforcement learning, but it can be a difficult area to dive into. As with much of machine learning, part of the difficulty stems from the fact that same idea often appears under multiple names. For example, it's incredibly useful to understand that "the backpropogation algorithm" is really just gradient descent applied to neural networks!<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup>  Working through the math often reveals that a new idea is something we were already familiar with in a different guise.</p>
<p>In the domain of reinforcement learning, names like policy gradient, <em>REINFORCE</em>, and so forth are often used with little exposition, and it's not always easy to find a simple explanation for these terms. It turns out however, that one particular flavour of reinforcment learning, known as <em>policy gradient</em>, is especially approchable. Moreover, there is a cool connection to modern variational inference that perhaps helps to make both of these things more clear. </p>
<h2>Reinforcement Learning</h2>
<p>The basic idea of reinforcement learning is that we want to train an model to make a sequence of predictions, where after each prediction, we recieve a reward, and enter into a new state. In particular, assume we start at time <span class="math">\(t=0\)</span> in state <span class="math">\(s_0\)</span>. At each time step we will take some action <span class="math">\(a_t\)</span>. By taking that action, we will get some reward, <span class="math">\(r_t\)</span>, and end up in a new state, <span class="math">\(s_{t+1}\)</span>. By default we assume that the transition between states (the dynamics of the system) is governed by a stochastic process, which I will parameterize by <span class="math">\(\phi\)</span>,</p>
<div class="math">$$r_t~, s_{t+1} \sim p(r_t~, s_{t+1}~|~s_t~, a_t ~; \phi)$$</div>
<p>As it turns out, I won't have anything to say about <span class="math">\(\phi\)</span> here, but it helps to keep the dependence explicit for the sake of clarity. Obviously this general setting includes the case of deterministic dynamics as a special case.</p>
<p>To choose our actions, we will make use of a stochastic <em>policy</em>, <span class="math">\(\pi\)</span>, which assigns a probability to each possible action, as a function only of our current state, <span class="math">\(s_t\)</span>, and parameters, <span class="math">\(\theta\)</span>:</p>
<div class="math">$$a_t \sim \pi(a_t~|~s_t ~; \theta)$$</div>
<p>A <em>trajectory</em> is the name for a sequence of state, action, reward tuples   :</p>
<div class="math">$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T) $$</div>
<p>The probability of a trajectory is just the product of the original transition and policy probabilities:</p>
<div class="math">$$p(\tau ~; \theta, \phi) = \prod_{t=1}^T p(r_t~, s_{t+1} | s_t~, a_t ~; \phi) ~\pi(a_t | s_t ~; \theta) $$</div>
<p>Eventually we will end up in some terminal state, <span class="math">\(s_T\)</span>, at which point we will return the total reward, which is just the sum of the individual rewards accumulated along the way: <span class="math">\(R(\tau) = \sum_t ~ r_t\)</span></p>
<h2>Policy Gradient</h2>
<p>The goal of reinforcement learning is to learn a good policy, one that will maximize our <em>expected reward</em>, where the expectation is with respect to the trajectory.</p>
<div class="math">$$ \underset{\theta}{\max} ~ \mathbb{E}_{p(\tau | \theta ~, \phi)} [R(\tau)]$$</div>
<p>Some approaches to reinforcement learning involve more complex modeling of the internal dynamics of the system. <em>Policy gradient</em>, however, is a particularly simple approach. We will simply use sampled trajectories to try to adjust our policy so as to increase the expected reward using gradient ascent.</p>
<p>In particular, if we can compute a gradient, <span class="math">\(\hat g\)</span>, with respect to parameters <span class="math">\(\theta\)</span>, then we can simply use this to repeatedly adjust our parameters, according to some step size <span class="math">\(\alpha\)</span>:</p>
<div class="math">$$ \hat g = \nabla_{\theta} ~ \mathbb{E}_{p(\tau | \theta ~, \phi)} [R(\tau)]$$</div>
<div class="math">$$ \theta \leftarrow \theta + \alpha \hat g$$</div>
<h2>The REINFORCE trick</h2>
<p>This seems simple enough, but there are a couple of problems.</p>
<p>The first and most obvious problem is that computing the gradient analytically is intractable, because it depends on the exponentially large space of trajectories. This in itself is not so big a problem as it seems, however, because we borrow an idea from stochastic gradient descent, and simply use samples to <em>approximate</em> the gradient. </p>
<p>In this case, we could think of taking sample trajectories and use these to make a monte carlo approximation:</p>
<div class="math">$$ \tau_i \sim p(\tau ~;~ \theta, \phi), \quad i=1,\ldots,N$$</div>
<div class="math">$$ \hat g = \nabla_{\theta} ~ \mathbb{E}_{p(\tau | \theta ~, \phi)} [R(\tau)] \approx \frac{1}{N} \sum_{i=1}^N \nabla_{\theta} [R(\tau_i)]$$</div>
<p>There is an additional problem here, however. The only part of the gradient which was dependent on <span class="math">\(\theta\)</span> was distribution governing the expectation! The reward is only a function of the trajectory, which, once we've taken a sample, is independent of <span class="math">\(\theta\)</span>. Obviously, we can't differentiate with respect to <span class="math">\(\theta\)</span> if it doesn't appear in the equation (or rather, the gradient would be zero).</p>
<p>To get around this, <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf">Williams (1992)</a> proposed what has come to be known as the <em>REINFORCE</em>, also sometimes referred to as the "likelihood ratio trick". Specifically, we can make use of the fact that</p>
<div class="math">$$ \nabla_{\theta} p(z ~;~ \theta) = p(z ~;~ \theta)  \nabla_{\theta} \log p(z ; \theta)$$</div>
<p>(To convince yourself of this, just note that <span class="math">\(\nabla_{\theta} \log p(z ; \theta) = \frac{1}{p(z ; \theta)} \nabla_{\theta} p(z ; \theta)\)</span>; see <a href="https://dl.acm.org/citation.cfm?id=84552">Glynn, 1990</a> for details).</p>
<p>If we make use of this trick before doing a monte carlo approximation of the gradient, we get:</p>
<div class="math">\begin{align}
\nabla_{\theta} ~ \mathbb{E}_{p(\tau | \theta ~, \phi)} [R(\tau)] &amp;= \nabla_{\theta} \int_{\tau} p(\tau ; \theta, \phi) R[\tau] d\tau \\
&amp;= \int_{\tau} \nabla_{\theta} p(\tau ; \theta, \phi) R[\tau] d\tau \\
&amp;= \int_{\tau} p(\tau ; \theta, \phi) \nabla_{\theta}  \log p(\tau ; \theta, \phi) R[\tau] d\tau \\
&amp;= \mathbb{E}_{p(\tau | \theta ~, \phi)} [R(\tau) \nabla_{\theta}  \log p(\tau ; \theta, \phi)] \\ 
&amp;\approx \frac{1}{N} \sum_{i=1}^N [R(\tau_i) \nabla_{\theta}  \log p(\tau_i ; \theta, \phi)] 
\end{align}</div>
<p>In this particular setting, we might go so far as to approximate the gradient with a single sample trajectory. The beautiful thing is that we can do this simply by using our current policy to generate actions, while letting the system determine our rewards and future states in response to those actions. Moreover, because the probability of a trajectory factors into the policy component and the state transition component, our approximation of the gradient ends up being only a function of the gradient of the log of our policy! </p>
<p>Assuming that we now have a single sample trajectory, <span class="math">\(\tau\)</span>, on each iteration, our approximation becomes:</p>
<div class="math">\begin{align}
\nabla_{\theta} ~ \mathbb{E}_{p(\tau | \theta ~, \phi)} [R(\tau)] &amp;\approx [R(\tau) \nabla_{\theta}  \log p(\tau ; \theta, \phi)] \\
&amp;= R(\tau) \sum_{t=1}^T \nabla_{\theta} [\log p(a_t | s_t ~;~ \theta) + \log p(r_t, s_{t+1} ~;~ s_t, a_t, \phi)] \\
&amp;= R(\tau) \sum_{t=1}^T \nabla_{\theta} \log p(a_t | s_t ~;~ \theta) \\
\end{align}</div>
<p>Now, it turns out that this approximation will be very high variance, and so various further tricks, such as using a baseline (also known as "control variates" - basically, subtracting something from each reward, possibly a quantity which we learn as a function of each state), are needed to make this work. But, the basic idea is there.</p>
<p>Stepping back, observe that this algorithm works by randomly sampling a trajectory, and then adjusting the policy so as to make the sampled trajectory more likely <em>in propotion to the total recieved reward</em>. When stated in those terms, it is all the more intuitive why a baseline would be important, so that we can actually decrease the likelihood of actions that lead to worse-than-expected rewards. </p>
<p>It is also worth noting that this approch doesn't distinguish between good and bad actions on a trajectory; all the actions taken are rewarded or punished together. While this seems like a rather blunt instrument, it is nevertheless a core element of recent successes like DeepMind's Alpha Go.</p>
<h2>Variational Inference</h2>
<p>This post is already getting long, so I won't cover all the relevant background variational inference, but I can't resist going over the way in which recent developments in VI have exploited the exact same idea.</p>
<p>The general setting in VI is that we want to approximate some posterior, <span class="math">\(p(z|x ; \phi)\)</span>. We assume that computing this posterior exactly is intractable, so we will instead approximate it with an approximating (variational) distribution, <span class="math">\(q(z | x ; \theta)\)</span>, which we assume to have a more tractable form.</p>
<p>There are a number of ways to derive the variational objective, but I think the simplest is assuming that we want to minimize the KL divergence between these two distributions:</p>
<div class="math">$$\underset{\theta}{\min} \textrm{KL}(q(z|x;\theta)||p(z|x;\phi))$$</div>
<p>If we work through the math, we will eventually encounter a term like:</p>
<div class="math">$$\nabla_{\theta} \mathbb{E}_{q(z|x;\theta)}[\log p(x, z ; \phi)]$$</div>
<p>Already, this should be looking familiar. </p>
<p>At this point, there are actually a couple of options. If <span class="math">\(z\)</span> is continuous, then we can use a clever reparameterization trick (<a href="https://arxiv.org/abs/1312.6114">Kingma and Welling, 2014</a>; <a href="https://arxiv.org/abs/1401.4082">Rezende et al 2014</a>), where we can re-express the distribution of <span class="math">\(z\)</span> as a function of an auxilliary variable, <span class="math">\(\epsilon\)</span>, which allows us to differentiate with respect to <span class="math">\(\theta\)</span>, i.e.</p>
<div class="math">$$\epsilon \sim N(0, I)$$</div>
<div class="math">$$z = \mu(x; \theta) + \Sigma(x; \theta) \cdot \epsilon$$</div>
<p>For more details on this, see the original paper, or this excellent <a href="https://arxiv.org/abs/1606.05908">turorial</a> by Carl Doersch.</p>
<p>Here, however, we are more interested in something more similar to <em>REINFORCE</em>, which alternately goes under the name of "Black Box variational inference" (<a href="https://arxiv.org/abs/1401.0118">Raganath et al 2013</a>) or "Neural variational inference" (<a href="https://arxiv.org/abs/1402.0030">Mnih and Gregor, 2014</a>).</p>
<p>The idea is exactly the same as above. By using the log trick, we can preserve the dependence on <span class="math">\(\theta\)</span> and approxiamte the gradient with an expectation:</p>
<div class="math">\begin{align}
\nabla_{\theta} \mathbb{E}_{q(z|x;\theta)}[\log p(x, z ; \phi)] &amp;= \mathbb{E}_{q(z|x;\theta)} [\log p(x, z; \phi) \nabla_{\theta} \log q(z | x; \theta)] \\
&amp;\approx \frac{1}{N} \sum_{i=1}^N \log p(x, z^{(i)} ; \phi) \nabla_{\theta} \log q(z^{(i)} | x ; \theta)
\end{align}</div>
<p>As with reinforcement learning, there are some refinements needed in order to get this to work, and I encourage you to check out the papers, but this simply idea opens up a large space of possibilities, including using much more flexible variational distibutions that was previously possible, and more fully automated tools, such as <a href="http://edwardlib.org/">Edward</a>.</p>
<h2>Summary</h2>
<p>In summary, <em>REINFORCE</em> began as the name of a 25-year old algorithm, but has come to refer to the trick of re-expressing a gradient such that expectations can be approximated with monte carlo approximations. <em>Policy gradient</em> is just gradient descent applied to the expected reward in a reinforcement learning setting, and black box / neural VI use the same trick to optimize a variational distribution. </p>
<p>For an even better expalnation of the same idea, check out this great <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">blog post</a>
 by Shakir Muhammed!</p>
<h2>References</h2>
<ul>
<li><a href="http://www.scholarpedia.org/article/Policy_gradient_methods">http://www.scholarpedia.org/article/Policy_gradient_methods</a></li>
<li>Doersch C. Tutorial on Variational Autoencoders. 2016. <a href="https://arxiv.org/abs/1606.05908">link</a> </li>
<li>Glynn PW (1990). Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):75–84, 1990. <a href="https://dl.acm.org/citation.cfm?id=84552">link</a></li>
<li>Kingma DP and Welling M. Auto-Encoding Variational Bayes. <em>ICLR</em>, 2014.<a href="https://arxiv.org/abs/1312.6114">link</a></li>
<li>Mnih A and Gregor K. Neural variational inference and learning in belief networks. <em>ICML</em>, 2014. <a href="https://arxiv.org/abs/1402.0030">link</a></li>
<li>Raganath R, Gerrish S, Blei DM. Black Box Variational Inference. <em>AISTATS</em>, 2014. <a href="https://arxiv.org/abs/1401.0118">link</a></li>
<li>Rezende DJ, Mohamed S, and Wierstra, D. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. <em>ICML</em>, 2014. <a href="https://arxiv.org/abs/1401.4082">link</a></li>
<li>Williams RJ. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. In <em>Machine Learning</em>, 8:229-256, 1992. <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf">link</a></li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Obviously "gradient descent" is additional terminology that might not be familiar to everyone, but I would argue it is a more general idea with broader applicability.&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Dallas Card
    </span>
  </span>
<time datetime="2017-03-19T12:00:00-07:00" pubdate>Sun 19 March 2017</time>  <span class="categories">
    <a class='category' href='https://dallascard.github.io/category/archive.html'>Archive</a>
  </span>
  <span class="categories">
    <a class="category" href="https://dallascard.github.io/tag/reinforcement-learning.html">reinforcement learning</a>,    <a class="category" href="https://dallascard.github.io/tag/variational-inference.html">variational inference</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="https://dallascard.github.io/the-reinforce-trick.html">The REINFORCE trick</a>
      </li>
      <li class="post">
          <a href="https://dallascard.github.io/installing-python-on-stampede.html">Installing python on Stampede</a>
      </li>
      <li class="post">
          <a href="https://dallascard.github.io/building-this-blog.html">Building this Blog</a>
      </li>
    </ul>
  </section>

  <section>
  <h1>Tags</h1>
    <a href="https://dallascard.github.io/tag/python.html">python</a>,    <a href="https://dallascard.github.io/tag/meta.html">meta</a>,    <a href="https://dallascard.github.io/tag/reinforcement-learning.html">reinforcement learning</a>,    <a href="https://dallascard.github.io/tag/variational-inference.html">variational inference</a>  </section>



</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2016&ndash;2017  Dallas Card &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="https://dallascard.github.io/theme/js/modernizr-2.0.js"></script>
  <script src="https://dallascard.github.io/theme/js/ender.js"></script>
  <script src="https://dallascard.github.io/theme/js/octopress.js" type="text/javascript"></script>
</body>
</html>