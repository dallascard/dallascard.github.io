<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>The link between reinforcement learning and variational inference &mdash; Reasonable Explanations</title>
  <meta name="author" content="Dallas Card">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">Reasonable Explanations</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>


<ul class="main-navigation">
      <li class="active">
        <a href="/category/archive.html">Archive</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">The link between reinforcement learning and variational inference</h1>
    <p class="meta">
<time datetime="2016-11-29T12:00:00-08:00" pubdate>Tue 29 November 2016</time>    </p>
</header>

  <div class="entry-content"><h2>Background</h2>
<p>One of the unfortunate parts of machine learning today is that the terminology can very confusing, and often the same idea appears under different names. For example, it's incredibly useful to understand that "the backpropogation algorithk" is just gradient descent applied to neural networks! (Obviously "gradient descent" is additional terminology that might not be familiar to everyone, but I would argue it is a more general idea with broader applicability). Working through the math often reveals that a new idea is something we were already familiar with in a different guise.</p>
<p>There is a lot of excitement these days around reinforcement learning, and the same problem exists here. Names like policy gradient, REINFORCE, and so forth get used frequently, often with little exposition, and it's not always easy to find a simple explanation. Moreover, it turns out that in this case there is a nice connection to modern variational inference, so all together that seemed to merit a bog post.</p>
<h2>Reinforcement Learning</h2>
<p>Let's start with the basic setup for reinforcement learning. The first assumption is that we're working with sequences of discrete steps. In particular, assume we start at time <span class="math">\(t=0\)</span> in state <span class="math">\(s_0\)</span>. At each time step we will take some action <span class="math">\(a_t\)</span>. By taking that action, we will get some reward, <span class="math">\(r_t\)</span>, and end up in a new state, <span class="math">\(s_{t+1}\)</span>. By default we assume that the dynamics of this are given by a stochastic process, which I have parameterized by <span class="math">\(\phi\)</span>,</p>
<div class="math">$$r_t~, s_{t+1} \sim p(r_t~, s_{t+1} | s_t~, a_t ~; \phi)$$</div>
<p>As it turns out, we won't have anything to say about <span class="math">\(\phi\)</span> here, but it helps to keep the dependence explicit for the sake of clarity. Clearly, deterministic dynamics are a special case.</p>
<p>To choose an action, we will make use of a stochastic \textit{policy}, <span class="math">\(\pi\)</span>, which only depends on our current state, and involves parameters $\theta:</p>
<div class="math">$$a_t \sim \pi(a_t | s_t ~; \theta)$$</div>
<p>The states, actions, and rewards we receive will collectively make up a trajectory:</p>
<div class="math">$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, ...)$$</div>
<p>The probability of a trajectory is just the product of the original transition and policy probabilities:</p>
<div class="math">$$p(\tau ~; \theta, \phi) = \underset{t}{\pi} p(r_t~, s_{t+1} | s_t~, a_t ~; \phi) p(a_t | s_t ~; \theat) $$</div>
<p>Eventually we will end up in some final state, <span class="math">\(s_F\)</span>, at which point we will return <span class="math">\(R = \sum_t r_t\)</span>, the sum of all the rewards we received along the way.</p>
<h2>Policy Gradient</h2>
<p>The goal of reinforcement learning is to learn a good policy, one that will maximize our \textit{expected reward}, where the expectation is with respect to the trajectory.</p>
<div class="math">$$ \underset{\theta}{\max} ~ \mathbb{E}_{p(\tau | \theta ~, \phi)} [R(\tau)]$$</div>
<p>Some approaches involve more complex modeling of the internal dynamics of the system. \textit{Policy gradient}, however, is particularly simple. We will simply try to adjust our policy to increase the expected reward using gradient ascent.</p>
<p>In particular, if we can compute a gradient, then we will simply use this to adjust our parameters:</p>
<div class="math">$$ \hat g = \nabla_{\theta} \mathbb{E}_{p(\tau | \theta ~, \phi)} [R(\tau)]$$</div>
<div class="math">$$ \theta \leftarrow \theta + \alpha \hat g$$</div>
<p>asdf</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Dallas Card
    </span>
  </span>
<time datetime="2016-11-29T12:00:00-08:00" pubdate>Tue 29 November 2016</time>  <span class="categories">
    <a class='category' href='/category/archive.html'>Archive</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/the-link-between-reinforcement-learning-and-variational-inference.html">The link between reinforcement learning and variational inference</a>
      </li>
      <li class="post">
          <a href="/installing-python-on-stampede.html">Installing Python on Stampede</a>
      </li>
      <li class="post">
          <a href="/building-this-blog.html">Building this Blog</a>
      </li>
    </ul>
  </section>

  <section>
  <h1>Tags</h1>
    <a href="/tag/python.html">python</a>,    <a href="/tag/meta.html">meta</a>  </section>



</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2016  Dallas Card &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
</body>
</html>